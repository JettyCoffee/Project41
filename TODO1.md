1. 使用SGD动量因子和cosine学习率下降机制提升模型性能
2. RNN的loss很快回升，表明模型过早进入过拟合，需要调整参数
3. epoch轮数提升至100

修改数据预处理方法，删除原有的简单分词器，对RNN模型使用spaCy库进行分词，spaCy为英语（en_core_web_sm）和德语（de_core_news_sm）提供了成熟的预训练模型；对于Transformer模型使用子词分词是更优的选择。字节对编码（BPE）在这里非常有效在使用BPE时，我们不需要设置太大的词表。通常将合并操作数（merge operations）设置为10,000到32,000之间就足够了。这能让模型在有限的数据上学到更通用的语言表示。

在数据的清洗上，将所有文本转换为小写，构建一个词频门槛。比如，过滤掉在整个数据集中出现次数少于2次的词。将这些低频词替换为统一的未知词标记（<unk>）。这能让模型专注于学习那些最具代表性的词汇模式。


针对Multi30K数据集，我们需要考虑它包含的语言特点。这个数据集主要涉及英语、德语和法语的图像描述。英语的处理相对简单，单词之间有明确的空格。德语的情况则比较特殊。德语中存在大量的复合词。多个单词经常合并写成一个很长的词。如果仅仅依靠空格进行切分，德语的词汇量会变得非常巨大。这会导致很多词在训练数据中只出现一次。

基于spaCy的经典分词 对于初学者或使用RNN、LSTM这类较早期的模型，使用spaCy库进行分词是一个标准做法。spaCy为英语（en_core_web_sm）和德语（de_core_news_sm）提供了成熟的预训练模型。它不仅能根据空格切分，还能正确处理标点符号和特殊字符。spaCy能将词还原为词根形式。比如，它能把“running”统一识别为“run”。这对于Multi30K这种小规模数据集很有帮助。它能有效降低词表的稀疏性。许多经典的PyTorch教程在处理Multi30K时都采用了这种方法。

基于子词的现代分词（BPE） 如果你计划使用Transformer或更现代的架构，子词分词是更优的选择。字节对编码（BPE）在这里非常有效。Multi30K的数据量只有约三万句。数据量较小意味着模型很容易过拟合。BPE可以将德语的长复合词拆解成有意义的词根或片段。这直接解决了未登录词的问题。在使用BPE时，我们不需要设置太大的词表。通常将合并操作数（merge operations）设置为10,000到32,000之间就足够了。这能让模型在有限的数据上学到更通用的语言表示。

词表构建与清洗 无论采用哪种分词方式，都需要对文本进行规范化处理。将所有文本转换为小写是必要的步骤。这能进一步压缩词表的大小。考虑到Multi30K的句子通常描述日常场景，句子长度较短。我们可以设定一个最大长度阈值。同时，我们需要构建一个词频门槛。比如，过滤掉在整个数据集中出现次数少于2次的词。将这些低频词替换为统一的未知词标记（<unk>）。这能让模型专注于学习那些最具代表性的词汇模式。

同时删除原有的简单分词器



